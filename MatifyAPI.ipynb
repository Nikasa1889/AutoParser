{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from requests import Session\n",
    "import MatifyAPI\n",
    "import io\n",
    "sess = Session()\n",
    "categories = MatifyAPI.getCategories(sess, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Request products from all sub category\n",
    "allProducts = []\n",
    "nTotalProducts = 0\n",
    "for categoryId, categoryName, subCategories in categories:\n",
    "    print \"-------------------------------------------\"\n",
    "    products = MatifyAPI.getProducts (sess, categoryId, categoryName, verbose=True)\n",
    "    nTotalProducts += len(products)\n",
    "    allProducts.append([categoryName, products])\n",
    "    for subCategoryID, subCategoryName, _ in subCategories:\n",
    "        products = MatifyAPI.getProducts (sess, subCategoryID, subCategoryName, verbose=True)\n",
    "        nTotalProducts += len(products)\n",
    "        allProducts.append([subCategoryName, products])\n",
    "\n",
    "print \"Total Products: \"  + str(nTotalProducts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "productWithImages = MatifyAPI.filterProductWithImage (allProducts, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Download all product images to the corresponding folder\n",
    "from MatifyDataset import MatifyDataset\n",
    "matifyDataset = MatifyDataset(datasetDir = 'MatifyDataset/', leastExamples = 15, nShards = 1, \n",
    "                              nValidations = 20, randomSeed = 0, verbose = True);\n",
    "#matifyDataset.download();\n",
    "matifyDataset.convertToTfrecord()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"/notebooks/AutoParser/squeezenet/models/slim/\"))\n",
    "from datasets import dataset_utils\n",
    "_NUM_SHARDS = 1\n",
    "class ImageReader(object):\n",
    "    \"\"\"Helper class that provides TensorFlow image coding utilities.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initializes function that decodes RGB JPEG data.\n",
    "        self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n",
    "        self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n",
    "\n",
    "    def read_image_dims(self, sess, image_data):\n",
    "        image = self.decode_jpeg(sess, image_data)\n",
    "        return image.shape[0], image.shape[1]\n",
    "\n",
    "    def decode_jpeg(self, sess, image_data):\n",
    "        image = sess.run(self._decode_jpeg,\n",
    "                     feed_dict={self._decode_jpeg_data: image_data})\n",
    "        assert len(image.shape) == 3\n",
    "        assert image.shape[2] == 3\n",
    "        return image\n",
    "\n",
    "def _get_dataset_filename(dataset_dir, split_name, shard_id):\n",
    "    output_filename = 'matify_%s_%05d-of-%05d.tfrecord' % (\n",
    "        split_name, shard_id, _NUM_SHARDS)\n",
    "    return os.path.join(dataset_dir, output_filename)\n",
    "\n",
    "def _convert_dataset(split_name, filenames, class_names_to_ids, dataset_dir):\n",
    "    \"\"\"Converts the given filenames to a TFRecord dataset.\n",
    "    Args:\n",
    "    split_name: The name of the dataset, either 'train' or 'validation'.\n",
    "    filenames: A list of absolute paths to png or jpg images.\n",
    "    class_names_to_ids: A dictionary from class names (strings) to ids\n",
    "      (integers).\n",
    "    dataset_dir: The directory where the converted datasets are stored.\n",
    "    \"\"\"\n",
    "    assert split_name in ['train', 'validation']\n",
    "\n",
    "    num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        image_reader = ImageReader()\n",
    "\n",
    "        with tf.Session('') as sess:\n",
    "\n",
    "            for shard_id in range(_NUM_SHARDS):\n",
    "                output_filename = _get_dataset_filename(dataset_dir, split_name, shard_id)\n",
    "\n",
    "                with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n",
    "                    start_ndx = shard_id * num_per_shard\n",
    "                    end_ndx = min((shard_id+1) * num_per_shard, len(filenames))\n",
    "                    for i in range(start_ndx, end_ndx):\n",
    "                        sys.stdout.write('\\r>> Converting image %d/%d shard %d' % (i+1, len(filenames), shard_id))\n",
    "                        sys.stdout.flush()\n",
    "\n",
    "                        # Read the filename:\n",
    "                        image_data = tf.gfile.FastGFile(filenames[i], 'r').read()\n",
    "                        height, width = image_reader.read_image_dims(sess, image_data)\n",
    "\n",
    "                        class_name = os.path.basename(os.path.dirname(filenames[i]))\n",
    "                        class_id = class_names_to_ids[class_name]\n",
    "\n",
    "                        example = dataset_utils.image_to_tfexample(\n",
    "                            image_data, 'jpg', height, width, class_id)\n",
    "                        tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "    sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, convert the training and validation sets.\n",
    "_convert_dataset('train', training_filenames, class_names_to_ids,\n",
    "               datasetDir)\n",
    "_convert_dataset('validation', validation_filenames, class_names_to_ids,\n",
    "               datasetDir)\n",
    "\n",
    "# Finally, write the labels file:\n",
    "labels_to_class_names = dict(zip(range(len(class_names)), class_names))\n",
    "dataset_utils.write_label_file(labels_to_class_names, datasetDir)\n",
    "\n",
    "print('\\nFinished converting the Matify dataset!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
